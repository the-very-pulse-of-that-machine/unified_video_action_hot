from functools import partial

import numpy as np
from tqdm import tqdm
import scipy.stats as stats
import math
from einops import rearrange
import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint

from unified_video_action.model.autoregressive.hot_transformer_block import HOTTransformerBlock
from unified_video_action.model.autoregressive.diffusion_loss import DiffLoss
from unified_video_action.model.autoregressive.diffusion_action_loss import DiffActLoss


def mask_by_order(mask_len, order, bsz, seq_len, device):
    masking = torch.zeros(bsz, seq_len).to(device)
    masking = torch.scatter(
        masking,
        dim=-1,
        index=order[:, : mask_len.long()],
        src=torch.ones(bsz, seq_len).to(device),
    ).bool()
    return masking


class MAR(nn.Module):
    """Masked Autoencoder with VisionTransformer backbone"""

    def __init__(
        self,
        img_size=256,
        vae_stride=16,
        patch_size=1,
        encoder_embed_dim=1024,
        encoder_depth=16,
        encoder_num_heads=16,
        decoder_embed_dim=1024,
        decoder_depth=16,
        decoder_num_heads=16,
        mlp_ratio=4.0,
        norm_layer=nn.LayerNorm,
        vae_embed_dim=16,
        mask_ratio_min=0.7,
        label_drop_prob=0.1,
        attn_dropout=0.1,
        proj_dropout=0.1,
        diffloss_d=3,
        diffloss_w=1024,
        diffloss_act_d=3,
        diffloss_act_w=1024,
        num_sampling_steps="100",
        diffusion_batch_mul=4,
        grad_checkpointing=False,
        predict_video=True,
        act_diff_training_steps=1000,
        act_diff_testing_steps="100",
        action_model_params={},
        **kwargs
    ):
        super().__init__()

        self.task_name = kwargs["task_name"]
        self.different_history_freq = kwargs["different_history_freq"]
        self.use_history_action = kwargs["use_history_action"]
        self.action_mask_ratio = kwargs["action_mask_ratio"]
        self.use_proprioception = kwargs["use_proprioception"]
        self.predict_wrist_img = kwargs["predict_wrist_img"]
        self.predict_proprioception = kwargs["predict_proprioception"]
        self.n_frames = 4

        # ========= VAE and patchify specifics =========
        self.img_size = img_size
        self.vae_stride = vae_stride
        self.patch_size = patch_size
        self.seq_h = self.seq_w = img_size // vae_stride // patch_size
        self.seq_len = self.seq_h * self.seq_w
        self.token_embed_dim = vae_embed_dim * patch_size**2
        self.vae_embed_dim = vae_embed_dim
        self.grad_checkpointing = grad_checkpointing
        self.label_drop_prob = label_drop_prob

        # ========= Masked MAE =========
        # variant masking ratio, a left-half truncated Gaussian centered at 100% masking ratio with std 0.25
        self.mask_ratio_generator = stats.truncnorm(
            (mask_ratio_min - 1.0) / 0.25, 0, loc=1.0, scale=0.25
        )

        # ========= Projection =========
        # conditional frames
        self.z_proj_cond = nn.Linear(self.token_embed_dim, encoder_embed_dim, bias=True)

        # video frames
        self.z_proj = nn.Linear(self.token_embed_dim, encoder_embed_dim, bias=True)

        # wrist video frames
        if self.predict_wrist_img:
            self.z_proj_wrist = nn.Linear(
                self.token_embed_dim, encoder_embed_dim, bias=True
            )

        # action
        self.predict_action = action_model_params["predict_action"]
        act_dim = kwargs["shape_meta"]["action"]["shape"][0]

        self.action_proj_cond = nn.Linear(act_dim, encoder_embed_dim, bias=True)
        self.buffer_size_action = 64

        # ========= Fake Latent =========
        self.fake_latent_x = nn.Parameter(torch.zeros(1, encoder_embed_dim))
        self.fake_action_latent = nn.Parameter(torch.zeros(1, encoder_embed_dim))
        if self.predict_wrist_img:
            self.fake_latent_wrist_x = nn.Parameter(torch.zeros(1, encoder_embed_dim))
        if self.use_history_action:
            self.fake_latent_history_action = nn.Parameter(
                torch.zeros(1, encoder_embed_dim)
            )

        # ========= History Action =========
        if self.use_history_action:
            self.history_action_proj_cond = nn.Linear(
                act_dim, encoder_embed_dim, bias=True
            )

        # ========= Proprioception =========
        if self.use_proprioception:
            self.buffer_size_properception = 64
            if self.different_history_freq:
                self.buffer_size_properception = 64 * 4

            if self.task_name == "umi":
                self.proprioception_proj_cond = nn.Linear(
                    16, encoder_embed_dim, bias=True
                )
            elif "pusht" in self.task_name:
                self.proprioception_proj_cond = nn.Linear(
                    2, encoder_embed_dim, bias=True
                )
            else:
                self.proprioception_proj_cond = nn.Linear(
                    9, encoder_embed_dim, bias=True
                )

            self.proprioception_image_proj_cond = nn.Linear(
                self.token_embed_dim, encoder_embed_dim, bias=True
            )

        # ========= Language Embedding =========
        self.language_emb_model = kwargs["language_emb_model"]
        self.language_emb_model_type = 1

        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                self.fake_latent = nn.Parameter(torch.zeros(1, encoder_embed_dim))
                self.text_proj_cond = nn.Linear(
                    512, encoder_embed_dim, bias=True
                )  # clip text embedding is 512
                self.buffer_size_text = 64
                self.text_pos_embed = nn.Parameter(
                    torch.zeros(1, self.buffer_size_text, encoder_embed_dim)
                )

        # ========= Projection =========
        if self.predict_wrist_img:
            proj_cond_x_dim_num = 4
            if self.use_proprioception:
                proj_cond_x_dim_num += 2
            if self.use_history_action:
                proj_cond_x_dim_num += 1
        else:
            proj_cond_x_dim_num = 3
            if self.use_proprioception:
                if (
                    self.task_name == "umi"
                    or "block_push" in self.task_name
                    or "pusht" in self.task_name
                ):
                    proj_cond_x_dim_num += 1
                else:
                    proj_cond_x_dim_num += 2
            if self.use_history_action:
                proj_cond_x_dim_num += 1

        self.proj_cond_x_layer = nn.Linear(
            proj_cond_x_dim_num * encoder_embed_dim, encoder_embed_dim, bias=True
        )

        # ========= Temporal and Spatial Position Embedding =========
        self.temporal_pos_embed = nn.Parameter(
            torch.zeros(1, self.n_frames, encoder_embed_dim)
        )  # Temporal position embedding, 4 frames
        self.spatial_pos_embed = nn.Parameter(
            torch.zeros(1, self.seq_len, encoder_embed_dim)
        )  # Spatial position embedding

        # ========= Normalization =========
        self.z_proj_ln = nn.LayerNorm(encoder_embed_dim, eps=1e-6)

        # ========= Encoder Blocks =========
        self.encoder_blocks = nn.ModuleList(
            [
                HOTTransformerBlock(
                    encoder_embed_dim,
                    encoder_num_heads,
                    mlp_ratio,
                    qkv_bias=True,
                    norm_layer=norm_layer,
                    proj_drop=proj_dropout,
                    attn_drop=attn_dropout,
                    use_hot=True
                )
                for _ in range(encoder_depth)
            ]
        )
        self.encoder_norm = norm_layer(encoder_embed_dim)

        # ========= Decoder =========
        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)

        # ========= Decoder Position Embedding =========
        self.decoder_temporal_pos_embed = nn.Parameter(
            torch.zeros(1, self.n_frames, decoder_embed_dim)
        )
        self.decoder_spatial_pos_embed = nn.Parameter(
            torch.zeros(1, self.seq_len, decoder_embed_dim)
        )

        # ========= Decoder Text Position Embedding =========
        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                self.decoder_text_pos_embed = nn.Parameter(
                    torch.zeros(1, self.buffer_size_text, decoder_embed_dim)
                )

        # ========= Decoder Blocks =========
        self.decoder_blocks = nn.ModuleList(
            [
                HOTTransformerBlock(
                    decoder_embed_dim,
                    decoder_num_heads,
                    mlp_ratio,
                    qkv_bias=True,
                    norm_layer=norm_layer,
                    proj_drop=proj_dropout,
                    attn_drop=attn_dropout,
                    use_hot=True
                )
                for _ in range(decoder_depth)
            ]
        )

        # ========= Decoder Norm =========
        self.decoder_norm = norm_layer(decoder_embed_dim)

        # ========= Diffusion Temporal and Spatial Embedding =========
        self.diffusion_temporal_embed = nn.Parameter(
            torch.zeros(1, self.n_frames, decoder_embed_dim)
        )
        self.diffusion_spatial_embed = nn.Parameter(
            torch.zeros(1, self.seq_len, decoder_embed_dim)
        )

        # ========= Initialize Weights =========
        self.initialize_weights()

        # ========= Video Diffusion Loss =========
        self.predict_video = predict_video
        if self.predict_video:
            # ========= Video Diffusion Loss =========
            self.diffloss = DiffLoss(
                target_channels=self.token_embed_dim,
                z_channels=decoder_embed_dim,
                width=diffloss_w,
                depth=diffloss_d,
                num_sampling_steps=num_sampling_steps,
                grad_checkpointing=grad_checkpointing,
                n_frames=self.n_frames,
                language_emb_model=self.language_emb_model,
                language_emb_model_type=self.language_emb_model_type,
            )

            # ========= Wrist Video Diffusion Loss =========
            if self.predict_wrist_img:
                self.diffloss_wrist = DiffLoss(
                    target_channels=self.token_embed_dim,
                    z_channels=decoder_embed_dim,
                    width=diffloss_w,
                    depth=diffloss_d,
                    num_sampling_steps=num_sampling_steps,
                    grad_checkpointing=grad_checkpointing,
                    n_frames=self.n_frames,
                    language_emb_model=self.language_emb_model,
                    language_emb_model_type=self.language_emb_model_type,
                )

        # ========= Action Diffusion Loss =========
        if self.predict_action:
            self.diffactloss = DiffActLoss(
                target_channels=act_dim,
                z_channels=decoder_embed_dim,
                width=diffloss_act_w,
                depth=diffloss_act_d,
                num_sampling_steps=num_sampling_steps,
                grad_checkpointing=grad_checkpointing,
                n_frames=self.n_frames,
                act_model_type=action_model_params["act_model_type"],
                act_diff_training_steps=act_diff_training_steps,
                act_diff_testing_steps=act_diff_testing_steps,
                language_emb_model=self.language_emb_model,
                language_emb_model_type=self.language_emb_model_type,
            )

        
        # ========= Proprioception Diffusion Loss =========
        if self.predict_proprioception:
            if self.task_name == "umi":
                self.diffproploss = DiffActLoss(
                    target_channels=6,
                    z_channels=decoder_embed_dim,
                    width=diffloss_act_w,
                    depth=diffloss_act_d,
                    num_sampling_steps=num_sampling_steps,
                    grad_checkpointing=grad_checkpointing,
                    n_frames=self.n_frames,
                    act_model_type=action_model_params["act_model_type"],
                    act_diff_training_steps=act_diff_training_steps,
                    act_diff_testing_steps=act_diff_testing_steps,
                    language_emb_model=self.language_emb_model,
                    language_emb_model_type=self.language_emb_model_type,
                )
            elif self.task_name == 'toolhang':
                self.diffproploss = DiffActLoss(
                        target_channels=9,
                        z_channels=decoder_embed_dim,
                        width=diffloss_act_w,
                        depth=diffloss_act_d,
                        num_sampling_steps=num_sampling_steps,
                        grad_checkpointing=grad_checkpointing,
                        n_frames=self.n_frames,
                        act_model_type=action_model_params["act_model_type"],
                        act_diff_training_steps=act_diff_training_steps,
                        act_diff_testing_steps=act_diff_testing_steps,
                        language_emb_model=self.language_emb_model,
                        language_emb_model_type=self.language_emb_model_type,
                    )
            else:
                raise NotImplementedError
            

    def initialize_weights(self):
        # parameters
        torch.nn.init.normal_(self.fake_latent_x, std=0.02)
        torch.nn.init.normal_(self.fake_action_latent, std=0.02)

        if self.predict_wrist_img:
            torch.nn.init.normal_(self.fake_latent_wrist_x, std=0.02)

        if self.use_history_action:
            torch.nn.init.normal_(self.fake_latent_history_action, std=0.02)

        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                torch.nn.init.normal_(self.fake_latent, std=0.02)

        torch.nn.init.normal_(self.temporal_pos_embed, std=0.02)
        torch.nn.init.normal_(self.spatial_pos_embed, std=0.02)

        torch.nn.init.normal_(self.decoder_temporal_pos_embed, std=0.02)
        torch.nn.init.normal_(self.decoder_spatial_pos_embed, std=0.02)

        torch.nn.init.normal_(self.diffusion_temporal_embed, std=0.02)
        torch.nn.init.normal_(self.diffusion_spatial_embed, std=0.02)

        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                torch.nn.init.normal_(self.text_pos_embed, std=0.02)
                torch.nn.init.normal_(self.decoder_text_pos_embed, std=0.02)

        # initialize nn.Linear and nn.LayerNorm
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
            if m.weight is not None:
                nn.init.constant_(m.weight, 1.0)

    def patchify(self, x):
        bsz, c, h, w = x.shape
        p = self.patch_size
        h_, w_ = h // p, w // p

        x = x.reshape(bsz, c, h_, p, w_, p)
        x = torch.einsum("nchpwq->nhwcpq", x)
        x = x.reshape(bsz, h_ * w_, c * p**2)
        return x  # [n, l, d]

    def unpatchify(self, x):
        bsz = x.shape[0]
        p = self.patch_size
        c = self.vae_embed_dim
        h_, w_ = self.seq_h, self.seq_w

        x = x.reshape(bsz, h_, w_, c, p, p)
        x = torch.einsum("nhwcpq->nchpwq", x)
        x = x.reshape(bsz, c, h_ * p, w_ * p)
        return x  # [n, c, h, w]

    def sample_orders(self, bsz):
        # generate a batch of random generation orders
        orders = []
        for _ in range(bsz):
            order = np.array(list(range(self.seq_len)))
            np.random.shuffle(order)
            orders.append(order)
        orders = torch.Tensor(np.array(orders)).to(self.device).long()
        return orders

    def random_masking(self, x, orders):
        # generate token mask
        bsz, t, seq_len, embed_dim = x.shape

        mask_rate = self.mask_ratio_generator.rvs(1)[0]

        num_masked_tokens = int(np.ceil(seq_len * mask_rate))
        mask = torch.zeros(bsz, t, seq_len, device=x.device)
        # Create the spatial mask for one frame (t=1)
        spatial_mask = torch.zeros(bsz, seq_len, device=x.device)
        spatial_mask = torch.scatter(
            spatial_mask,
            dim=-1,
            index=orders[:, :num_masked_tokens],
            src=torch.ones(bsz, seq_len, device=x.device),
        )
        # Expand the spatial mask to all frames in the video
        mask = spatial_mask.unsqueeze(1).expand(-1, t, -1)

        return mask
    

    def forward_mae_encoder(
        self,
        x,                      # ËæìÂÖ•ËßÜÈ¢ëÁâπÂæÅ: [B, T, S, C]
        mask,                   # Êé©Á†Å: [B, T, S]
        cond,                   # Êù°‰ª∂ÂõæÂÉè patch: [B, T, S, C]
        text_latents=None,      # ÊñáÊú¨ latent
        history_nactions=None,  # ÂéÜÂè≤Âä®‰ΩúÔºàÂèØÈÄâÔºâ
        nactions=None,          # ÂΩìÂâçÂä®‰ΩúÔºàÂèØÈÄâÔºâ
        task_mode=None,         # ‰ªªÂä°Ê®°Âºè: policy_model / inverse_model / video_model ‚Ä¶
        proprioception_input={},# Êú¨‰ΩìÊÑüÁü•ËæìÂÖ•
    ):
        # x shape: [B, T, S, C]
        B, T, S, _ = x.size()

        # Êé©Á†ÅÂ±ïÂºÄ: [B, T, S] ‚Üí [B, T*S]
        mask = rearrange(mask, "b t s -> b (t s)")

        # ---------------------------------------------------------------------
        # üü¶ 1. Ê†πÊçÆ‰ªªÂä°Ê®°ÂºèÂ§ÑÁêÜ xÔºàËßÜÈ¢ë latentÔºâ‰∏é condÔºàÊù°‰ª∂ latentÔºâ
        # ---------------------------------------------------------------------
        if task_mode == "policy_model":
            # cond: ÊäïÂΩ± + Â±ïÂπ≥‰∏∫ [B, T*S, C]
            cond = self.z_proj_cond(cond)
            cond = rearrange(cond, "b t s c -> b (t s) c")

            # Á≠ñÁï•Ê®°Âûã‰∏ç‰ΩøÁî®ÂéüÂßã xÔºåÁî® fake latent Êõø‰ª£
            x = self.fake_latent_x.unsqueeze(1).expand(B, cond.size(1), -1)

            # Â¶ÇÊûúÈ¢ÑÊµãËÖïÈÉ®ËßÜÈ¢ëÔºåwrist_x ‰πü‰ΩøÁî® fake latent
            if self.predict_wrist_img:
                wrist_x = self.fake_latent_wrist_x.unsqueeze(1).expand(
                    B, cond.size(1), -1
                )

        elif task_mode == "inverse_model":
            # inverse model ‰ΩøÁî®ËæìÂÖ•ËßÜÈ¢ë x
            x = self.z_proj(x)
            x = rearrange(x, "b t s c -> b (t s) c")

            # wrist ÂõæÂÉè‰πüÊäïÂΩ±Êàê latent
            if self.predict_wrist_img:
                wrist_x = self.z_proj_wrist(proprioception_input["pred_second_image_z"])
                wrist_x = rearrange(wrist_x, "b t s c -> b (t s) c")

            # cond ÂèçËøáÊù•‰ΩøÁî® fake latent
            cond = self.fake_latent_x.unsqueeze(1).expand(B, x.size(1), -1)

        else:
            # video_model / dynamic_model / full_dynamic_model Á≠â
            cond = self.z_proj_cond(cond)
            cond = rearrange(cond, "b t s c -> b (t s) c")

            # x ‰πüÊäïÂΩ±
            x = self.z_proj(x)
            x = rearrange(x, "b t s c -> b (t s) c")

            # mask ‰ΩçÁΩÆÊõøÊç¢‰∏∫ fake latentÔºàMAE ÁöÑ mask Êú∫Âà∂Ôºâ
            fake_latent_expanded = self.fake_latent_x.unsqueeze(1).expand(
                B, x.size(1), -1
            )
            x[mask == 1] = fake_latent_expanded[mask == 1].to(x.dtype)

            # wrist ‰πüÊâßË°åÂêåÊ†∑ÁöÑ masking
            if self.predict_wrist_img:
                wrist_x = self.z_proj_wrist(proprioception_input["pred_second_image_z"])
                wrist_x = rearrange(wrist_x, "b t s c -> b (t s) c")
                fake_wrist_latent_expanded = self.fake_latent_wrist_x.unsqueeze(
                    1
                ).expand(B, wrist_x.size(1), -1)
                wrist_x[mask == 1] = fake_wrist_latent_expanded[mask == 1].to(
                    wrist_x.dtype
                )

        embed_dim = cond.size(2)

        # ---------------------------------------------------------------------
        # üü¶ 2. ÂéÜÂè≤Âä®‰ΩúÁºñÁ†ÅÔºàÂèØÈÄâÔºâ
        # ---------------------------------------------------------------------
        if self.use_history_action:
            if history_nactions is None:
                # Êó†ÂéÜÂè≤Âä®‰Ωú ‚Üí Áî® fake latent
                history_action_latents = self.fake_latent_history_action.unsqueeze(
                    0
                ).repeat(B, T * self.n_frames, 1)
            else:
                # ÊúâÂéÜÂè≤Âä®‰Ωú ‚Üí ÊäïÂΩ±
                history_action_latents = self.history_action_proj_cond(history_nactions)

                # ËÆ≠ÁªÉÊó∂ÈöèÊú∫ mask Âä®‰Ωú
                if self.training:
                    history_action_mask = (
                        torch.rand(B, T * self.n_frames) > self.action_mask_ratio
                    ).int()
                    history_action_latents[history_action_mask == 1] = (
                        self.fake_latent_history_action.to(history_action_latents.dtype)
                    )

            # Êâ©Â±ïÂà∞ bufferÔºàÈáçÂ§çÂ§öÊ¨°Ôºâ
            history_action_latents_expand = history_action_latents.repeat_interleave(
                self.buffer_size_action, dim=1
            )

        # ---------------------------------------------------------------------
        # üü¶ 3. Êú¨‰ΩìÊÑüÁü•ÔºàProprioceptionÔºâÈÉ®ÂàÜ
        # ---------------------------------------------------------------------
        if self.use_proprioception:
            if self.task_name == "umi":
                # UMI robot ÁâπÊúâÁöÑ state ÊûÑÊàê
                proprioception_state_cond = torch.cat(
                    [
                        proprioception_input["robot0_eef_pos"],
                        proprioception_input["robot0_eef_rot_axis_angle"],
                        proprioception_input["robot0_gripper_width"],
                        proprioception_input["robot0_eef_rot_axis_angle_wrt_start"],
                    ],
                    dim=-1,
                )
                proprioception_state_cond = self.proprioception_proj_cond(
                    proprioception_state_cond.float()
                )
                proprioception_state_cond_expand = (
                    proprioception_state_cond.repeat_interleave(
                        self.buffer_size_properception, dim=1
                    )
                )
            else:
                # ËßÜËßâÊú¨‰Ωì‰ø°ÊÅØ second_image_z
                proprioception_image_cond = self.proprioception_image_proj_cond(
                    proprioception_input["second_image_z"]
                )
                proprioception_image_cond = rearrange(
                    proprioception_image_cond, "b t s c -> b (t s) c"
                )

                # eef pos + quat + gripper
                proprioception_state_cond = torch.cat(
                    [
                        proprioception_input["robot0_eef_pos"],
                        proprioception_input["robot0_eef_quat"],
                        proprioception_input["robot0_gripper_qpos"],
                    ],
                    dim=-1,
                )
                proprioception_state_cond = self.proprioception_proj_cond(
                    proprioception_state_cond
                )
                proprioception_state_cond_expand = (
                    proprioception_state_cond.repeat_interleave(
                        self.buffer_size_properception, dim=1
                    )
                )

        # ---------------------------------------------------------------------
        # üü¶ 4. Âä®‰Ωú latent
        # ---------------------------------------------------------------------
        if task_mode == "dynamic_model":
            action_latents = self.action_proj_cond(nactions)
        else:
            action_latents = self.fake_action_latent.unsqueeze(0).repeat(B, 16, 1)

        action_latents_expand = action_latents.repeat_interleave(
            self.buffer_size_action, dim=1
        )

        # ---------------------------------------------------------------------
        # üü¶ 5. ÊãºÊé•ÊâÄÊúâÊ®°ÊÄÅÔºàËßÜÈ¢ë„ÄÅcond„ÄÅÂä®‰Ωú„ÄÅÊú¨‰Ωì„ÄÅÂéÜÂè≤Âä®‰Ωú‚Ä¶Ôºâ
        # ---------------------------------------------------------------------
        if self.predict_wrist_img:
            parts = [x, wrist_x, cond]
            if self.use_history_action:
                parts.append(history_action_latents_expand)
            parts.append(action_latents_expand)
            if self.use_proprioception:
                parts.extend(
                    [proprioception_image_cond, proprioception_state_cond_expand]
                )
            x = torch.cat(parts, dim=-1)
        else:
            parts = [x, cond]
            if self.use_history_action:
                parts.append(history_action_latents_expand)
            parts.append(action_latents_expand)

            if self.use_proprioception:
                if self.task_name == "umi":
                    parts.append(proprioception_state_cond_expand)
                else:
                    parts.extend(
                        [proprioception_image_cond, proprioception_state_cond_expand]
                    )
            x = torch.cat(parts, dim=-1)

        # ---------------------------------------------------------------------
        # üü¶ 6. Ê®°ÊÄÅËûçÂêà Projection
        # ---------------------------------------------------------------------
        x = self.proj_cond_x_layer(x)

        # ---------------------------------------------------------------------
        # üü¶ 7. Êó∂Á©∫‰ΩçÁΩÆÁºñÁ†Å
        # ---------------------------------------------------------------------
        temporal_pos_embed_expanded = self.temporal_pos_embed.unsqueeze(2).expand(
            -1, -1, S, -1
        )
        spatial_pos_embed_expanded = self.spatial_pos_embed.unsqueeze(1).expand(
            -1, T, -1, -1
        )
        combined_pos_embed = (
            temporal_pos_embed_expanded + spatial_pos_embed_expanded
        ).reshape(-1, T * S, embed_dim)

        x = x + combined_pos_embed

        # ---------------------------------------------------------------------
        # üü¶ 8. ÊñáÊú¨ embedding ÊèíÂÖ•ÔºàÁî®‰∫éÊù°‰ª∂ÁîüÊàêÔºâ
        # ---------------------------------------------------------------------
        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                text_latents = text_latents.unsqueeze(1).repeat(
                    1, self.buffer_size_text, 1
                )

                # classifier free guidance (CFG)
                if self.training:
                    drop_latent_mask = torch.rand(B) < self.label_drop_prob
                    drop_latent_mask = (
                        drop_latent_mask.unsqueeze(-1).to(self.device).to(x.dtype)
                    )
                    drop_latent_mask = drop_latent_mask.unsqueeze(1).repeat(
                        1, self.buffer_size_text, 1
                    )
                    text_latents = (
                        drop_latent_mask
                        * self.fake_latent.unsqueeze(1).repeat(
                            1, self.buffer_size_text, 1
                        )
                        + (1 - drop_latent_mask) * text_latents
                    )

                text_latents = text_latents + self.text_pos_embed
                x = torch.cat([text_latents, x], dim=1)

        # ---------------------------------------------------------------------
        # üü¶ 9. LayerNorm
        # ---------------------------------------------------------------------
        x = self.z_proj_ln(x)

        # ---------------------------------------------------------------------
        # üü¶ 10. Transformer Encoder BlocksÔºàÂèØ checkpointÔºâ
        # ---------------------------------------------------------------------
        if self.grad_checkpointing and not torch.jit.is_scripting():
            for block in self.encoder_blocks:
                x = checkpoint(block, x)
        else:
            for block in self.encoder_blocks:
                x = block(x)

        # ÊúÄÁªàÁºñÁ†ÅÂêéÁöÑÂ∫èÂàó
        x = self.encoder_norm(x)

        return x


    def forward_mae_decoder(self, x, mask):
        # mask ÂΩ¢Áä∂‰∏∫ [B, T, S]
        B, T, S = mask.size()

        # Â±ïÂπ≥ mask: [B, T, S] -> [B, T*S]
        mask = rearrange(mask, "b t s -> b (t s)")

        # Â∞Ü encoder ËæìÂá∫ÁöÑÁâπÂæÅ x ÂÅö decoder ËæìÂÖ•ÊäïÂΩ±ÔºàÈôçÁª¥ÊàñÊç¢ÈÄöÈÅìÔºâ
        # x shape remains: [B, T*S (+ text), C]
        x = self.decoder_embed(x)
        _, _, embed_dim = x.shape

        # ---------------------------------------------------------------------
        # üü¶ 1. MAE Decoder ÁöÑÊó∂Á©∫‰ΩçÁΩÆÁºñÁ†ÅÔºà‰∏é Encoder Áã¨Á´ãÔºâ
        # ---------------------------------------------------------------------

        # decoder_temporal_pos_embedÔºöÂΩ¢Áä∂ [1, T, C]
        # Êâ©Â±ïÂà∞ [1, T, S, C]
        decoder_temporal_pos_embed_expanded = self.decoder_temporal_pos_embed.unsqueeze(
            2
        ).expand(
            -1, -1, S, -1
        )

        # decoder_spatial_pos_embedÔºöÂΩ¢Áä∂ [1, S, C]
        # Êâ©Â±ïÂà∞ [1, T, S, C]
        decoder_spatial_pos_embed_expanded = self.decoder_spatial_pos_embed.unsqueeze(
            1
        ).expand(
            -1, T, -1, -1
        )

        # ‰∏§ËÄÖÁõ∏Âä†ÔºåÂæóÂà∞ÊØè‰∏™ patch ÂØπÂ∫îÁöÑÊó∂Á©∫‰ΩçÁΩÆÁºñÁ†Å
        # reshape ‚Üí [1, T*S, C]
        decoder_combined_pos_embed = (
            decoder_temporal_pos_embed_expanded + decoder_spatial_pos_embed_expanded
        ).reshape(1, T * S, embed_dim)

        # ---------------------------------------------------------------------
        # üü¶ 2. Ëã•ÊúâÊñáÊú¨ embeddingÔºåÂàôÂú® decoder ‰πüË¶ÅÂ§ÑÁêÜ‰ΩçÁΩÆÁºñÁ†Å
        # ---------------------------------------------------------------------

        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                # text embedding ÁöÑ decoder ‰ΩçÁΩÆÁºñÁ†Å concat Âú®ÊúÄÂâç
                combined_pos_embed = torch.cat(
                    [self.decoder_text_pos_embed, decoder_combined_pos_embed],
                    dim=1,
                )
            else:
                combined_pos_embed = decoder_combined_pos_embed
        else:
            combined_pos_embed = decoder_combined_pos_embed

        # Âä†‰∏ä decoder ÁöÑ‰ΩçÁΩÆÁºñÁ†Å
        x = x + combined_pos_embed

        # ---------------------------------------------------------------------
        # üü¶ 3. Transformer Decoder Blocks
        # ---------------------------------------------------------------------
        if self.grad_checkpointing and not torch.jit.is_scripting():
            for block in self.decoder_blocks:
                x = checkpoint(block, x)
        else:
            for block in self.decoder_blocks:
                x = block(x)

        # Decoder Â±ÇÊ†áÂáÜÂåñ
        x = self.decoder_norm(x)

        # ---------------------------------------------------------------------
        # üü¶ 4. ÂâîÈô§ÊñáÊú¨ÈÉ®ÂàÜÔºàËã• decoder ÂâçÈù¢ÊãºÊé•‰∫ÜÊñáÊú¨ tokenÔºâ
        # ---------------------------------------------------------------------
        if self.language_emb_model == "clip":
            if self.language_emb_model_type == 1:
                # x Ââç buffer_size_text ‰∏™ token ÊòØÊñáÊú¨ÁâπÂæÅÔºå‰∏çÂèÇ‰∏éÈáçÂª∫
                # ‰øùÁïôÂêéÁª≠ÁöÑËßÜÈ¢ë token
                x = x[:, self.buffer_size_text:]

        # ---------------------------------------------------------------------
        # üü¶ 5. diffusionÔºàÊâ©Êï£Ê®°ÂûãÔºâÁî®ÁöÑÈ¢ùÂ§ñ‰ΩçÁΩÆÁºñÁ†Å
        # ---------------------------------------------------------------------
        # Êâ©Êï£Ê®°ÂûãÁî®Ëá™Â∑±ÁöÑÊó∂Á©∫ÁºñÁ†ÅÔºå‰∏é MAE ‰ΩçÁΩÆÁºñÁ†ÅÂàÜÁ¶ª

        diffusion_temporal_pos_embed_expanded = self.diffusion_temporal_embed.unsqueeze(
            2
        ).expand(
            -1, -1, S, -1
        )

        diffusion_spatial_pos_embed_expanded = self.diffusion_spatial_embed.unsqueeze(
            1
        ).expand(
            -1, T, -1, -1
        )

        # ÂΩ¢Áä∂: [1, T*S, C]
        diffusion_combined_pos_embed = (
            diffusion_temporal_pos_embed_expanded + diffusion_spatial_pos_embed_expanded
        ).reshape(1, T * S, embed_dim)

        # diffusion ‰ΩçÁΩÆÁºñÁ†ÅÁõ∏Âä†„ÄÇÊ≥®ÊÑèËøôÈáå‰∏çÊòØÊõøÊç¢ÔºåËÄåÊòØ‰∫åÊ¨°Âä†Êàê„ÄÇ
        x = x + diffusion_combined_pos_embed

        # decoder ËæìÂá∫Â∫èÂàó
        return x


    def forward_loss(
        self,
        z,                  # decoder ËæìÂá∫ÁöÑ latent Â∫èÂàóÔºàÈ¢ÑÊµãÁªìÊûúÔºâ
        target,             # ground truth ÈáçÂª∫ÁõÆÊ†áÔºàËßÜÈ¢ë patch latentÔºâ
        mask,               # mask: [B, T, S]ÔºåË°®Á§∫Ë¢´ mask ÁöÑ patch
        nactions=None,      # ground truth Âä®‰ΩúÔºàÁî®‰∫é policy / inverse / fullÔºâ
        task_mode=None,     # ÂΩìÂâç‰ªªÂä°Ê®°ÂºèÔºövideo / policy / inverse / full
        gt_wrist_latents=None,   # wrist camera ÁöÑ ground truth latent
        gt_properception=None,   # Êú¨‰ΩìÊÑüÁü• ground truth (eef pos/quat/gripper)
        text_latents=None,       # ÊñáÊú¨ latentÔºåÁî®‰∫éÊù°‰ª∂Êâ©Êï£ÊçüÂ§±
    ):
        # ----------------------------------------------------------------------
        # üü¶ 1. Video Model / Dynamic Model ‚Äî‚Äî Âè™ËÆ≠ÁªÉ videoÔºàÈ¢ÑÊµãÊú™Êù•Â∏ßÔºâ
        # ----------------------------------------------------------------------
        if task_mode == "video_model" or task_mode == "dynamic_model":

            if self.predict_wrist_img:
                # ‰∏ªËßÜËßíËßÜÈ¢ëÁöÑ diffusion ÈáçÂª∫ÊçüÂ§±
                video_loss = self.diffloss(
                    z=z,
                    target=target,
                    mask=mask,
                    text_latents=text_latents,
                )
                # ËÖïÈÉ®Áõ∏Êú∫ËßÜÈ¢ëÁöÑ diffusion ÈáçÂª∫ÊçüÂ§±ÔºàÈ¢ùÂ§ñÂàÜÊîØÔºâ
                video_loss_wrist = self.diffloss_wrist(
                    z=z,
                    target=gt_wrist_latents,
                    mask=mask,
                    text_latents=text_latents,
                )
                # ‰∏§ËÄÖÁõ∏Âä†ÔºöÊúÄÁªàËßÜÈ¢ëÊçüÂ§±
                video_loss = video_loss + video_loss_wrist
            else:
                # Ê≤°Êúâ wrist cameraÔºåÂè™Êúâ‰∏ªËßÜÈ¢ëÊçüÂ§±
                video_loss = self.diffloss(
                    z=z,
                    target=target,
                    mask=mask,
                    text_latents=text_latents,
                )

            # video model ‰∏çËÆ≠ÁªÉÂä®‰Ωú
            act_loss = torch.tensor(0.0).to(self.device)

            # ÊÄªÊçüÂ§± = video loss
            loss = video_loss

        # ----------------------------------------------------------------------
        # üü¶ 2. Policy Model / Inverse Model ‚Äî‚Äî Âè™ËÆ≠ÁªÉÂä®‰ΩúÔºå‰∏çËÆ≠ÁªÉËßÜÈ¢ë
        # ----------------------------------------------------------------------
        elif task_mode == "policy_model" or task_mode == "inverse_model":

            # diffusion-based Âä®‰ΩúÈ¢ÑÊµãÊçüÂ§±
            act_loss = self.diffactloss(
                z=z,
                target=nactions,
                task_mode=task_mode,
                text_latents=text_latents,
            )

            # ‰∏çÂÅöËßÜÈ¢ëÈáçÂª∫
            video_loss = torch.tensor(0.0).to(self.device)

            # ÊÄªÊçüÂ§±Âè™ÂåÖÂê´Âä®‰ΩúÊçüÂ§±
            loss = act_loss

        # ----------------------------------------------------------------------
        # üü¶ 3. Full Dynamic Model ‚Äî‚Äî ÂêåÊó∂È¢ÑÊµãËßÜÈ¢ëÔºàÊú™Êù•Â∏ßÔºâ+ Âä®‰Ωú
        # ----------------------------------------------------------------------
        elif task_mode == "full_dynamic_model":

            # ËßÜÈ¢ëÈÉ®ÂàÜ
            if self.predict_wrist_img:
                video_loss = self.diffloss(
                    z=z,
                    target=target,
                    mask=mask,
                    text_latents=text_latents,
                )
                video_loss_wrist = self.diffloss_wrist(
                    z=z,
                    target=gt_wrist_latents,
                    mask=mask,
                    text_latents=text_latents,
                )
                video_loss = video_loss + video_loss_wrist
            else:
                video_loss = self.diffloss(
                    z=z,
                    target=target,
                    mask=mask,
                    text_latents=text_latents,
                )

            # Âä®‰ΩúÈÉ®ÂàÜ
            act_loss = self.diffactloss(
                z=z,
                target=nactions,
                task_mode=task_mode,
                text_latents=text_latents,
            )

            # full model = video_loss + action_loss
            loss = video_loss + act_loss

        # ----------------------------------------------------------------------
        # üü¶ 4. Ëã•ÈúÄË¶ÅÈ¢ÑÊµãÊú¨‰ΩìÊÑüÁü• proprioceptionÔºåÂàôÂä†ÂÖ•È¢ùÂ§ñÊçüÂ§±
        # ----------------------------------------------------------------------
        if self.predict_proprioception:
            properception_loss = self.diffproploss(
                z=z,
                target=gt_properception,
                text_latents=text_latents,
            )
            loss = loss + properception_loss

        # ËøîÂõûÔºöÊÄªÊçüÂ§± + video_loss + act_loss
        return loss, video_loss, act_loss

    def forward(
        self,
        imgs,                       # ËæìÂÖ•ËßÜÈ¢ë [B, T, C, H, W]
        cond,                       # Êù°‰ª∂ËßÜÈ¢ëÔºàÈÄöÂ∏∏ÊòØËßÇÊµãÔºâ[B, T, C, H, W]
        history_nactions=None,      # ÂéÜÂè≤Âä®‰Ωú
        nactions=None,              # ÂΩìÂâçÂä®‰ΩúÔºàÁî®‰∫é actor Êàñ inverseÔºâ
        text_latents=None,          # ÊñáÊú¨ latent
        task_mode=None,             # ÂΩìÂâç‰ªªÂä°Ê®°Âºè
        proprioception_input={},    # Êú¨‰ΩìÊÑüÁü•‰ø°ÊÅØÔºàeef pos/quat/gripperÔºâ
    ):
        # ÂΩìÂâç batch ÁöÑ device
        self.device = cond.device

        # imgs: [B, T, C, H, W]
        B, T, C, H, W = imgs.size()

        # ----------------------------------------------------------------------
        # üü¶ 1. Patchify ËßÜÈ¢ëËæìÂÖ• imgs
        # ----------------------------------------------------------------------

        # Â±ïÂπ≥ batch Âíå time: [B*T, C, H, W]
        imgs = rearrange(imgs, "b t c h w -> (b t) c h w")

        # patchify: -> [B*T, S, C']  (S=patchÊï∞)
        x = self.patchify(imgs)

        # ÂÜç reshape Âõû‰∏âÁª¥ time: [B, T, S, C']
        x = rearrange(x, "(b t) seq_len c -> b t seq_len c", b=B)

        # ----------------------------------------------------------------------
        # üü¶ 2. Patchify Êù°‰ª∂ condÔºà‰∏é‰∏äÈù¢Á±ª‰ººÔºâ
        # ----------------------------------------------------------------------
        cond = rearrange(cond, "b t c h w -> (b t) c h w")
        cond = self.patchify(cond)
        cond = rearrange(cond, "(b t) seq_len c -> b t seq_len c", b=B)

        # ----------------------------------------------------------------------
        # üü¶ 3. Â§ÑÁêÜ proprioceptionÔºàËã•‰ΩøÁî®Á¨¨‰∫åÊëÑÂÉèÂ§¥Ôºâ
        # ----------------------------------------------------------------------
        if self.use_proprioception:
            if "second_image_z" in proprioception_input:

                # patchify Á¨¨‰∫åÁõ∏Êú∫ÂõæÂÉè
                proprioception_input["second_image_z"] = rearrange(
                    proprioception_input["second_image_z"], "b t c h w -> (b t) c h w"
                )
                proprioception_input["second_image_z"] = self.patchify(
                    proprioception_input["second_image_z"]
                )
                proprioception_input["second_image_z"] = rearrange(
                    proprioception_input["second_image_z"],
                    "(b t) seq_len c -> b t seq_len c",
                    b=B,
                )

        # ----------------------------------------------------------------------
        # üü¶ 4. Ëã•ÈúÄË¶ÅÈ¢ÑÊµã Wrist CameraÔºåÂàô patchify wrist camera È¢ÑÊµãÂõæÂÉè
        # ----------------------------------------------------------------------
        if self.predict_wrist_img:
            if "pred_second_image_z" in proprioception_input:
                proprioception_input["pred_second_image_z"] = rearrange(
                    proprioception_input["pred_second_image_z"],
                    "b t c h w -> (b t) c h w",
                )
                proprioception_input["pred_second_image_z"] = self.patchify(
                    proprioception_input["pred_second_image_z"]
                )
                proprioception_input["pred_second_image_z"] = rearrange(
                    proprioception_input["pred_second_image_z"],
                    "(b t) seq_len c -> b t seq_len c",
                    b=B,
                )

        # ----------------------------------------------------------------------
        # üü¶ 5. ÊñáÊú¨ latent Â§ÑÁêÜÔºàÂ¶Ç‰ΩøÁî® text_proj_condÔºâ
        # ----------------------------------------------------------------------
        if text_latents is not None and hasattr(self, "text_proj_cond"):
            if self.language_emb_model_type == 1:
                text_latents = self.text_proj_cond(text_latents)

        # ground truth latentÔºàËßÜÈ¢ë patch latentÔºâ
        gt_latents = x.clone().detach()

        # wrist camera GT latent
        if self.predict_wrist_img:
            if "pred_second_image_z" in proprioception_input:
                gt_wrist_latents = proprioception_input["pred_second_image_z"].clone().detach()
                gt_wrist_latents = rearrange(gt_wrist_latents, "b t s c -> b (t s) c")

        # ----------------------------------------------------------------------
        # üü¶ 6. ÈöèÊú∫ MaskÔºàMAEÔºâÁîüÊàê mask
        # ----------------------------------------------------------------------
        # orders shape example: [B, T, S] ÁöÑÈöèÊú∫ÈÅçÂéÜÈ°∫Â∫è
        orders = self.sample_orders(bsz=B)

        # mask: [B, T, S]Ôºà1 = maskedÔºå0 = keepÔºâ
        mask = self.random_masking(x, orders)

        # ----------------------------------------------------------------------
        # üü¶ 7. MAE Encoder
        # ----------------------------------------------------------------------
        x = self.forward_mae_encoder(
            x,
            mask,
            cond,
            text_latents,
            history_nactions,
            nactions,
            task_mode=task_mode,
            proprioception_input=proprioception_input,
        )

        # ----------------------------------------------------------------------
        # üü¶ 8. MAE Decoder
        # ----------------------------------------------------------------------
        z = self.forward_mae_decoder(x, mask)

        # ----------------------------------------------------------------------
        # üü¶ 9. reshape mask / gt (flatten: [B, T, S] ‚Üí [B, T*S])
        # ----------------------------------------------------------------------
        mask = rearrange(mask, "b t s -> b (t s)")
        gt_latents = rearrange(gt_latents, "b t s c -> b (t s) c")

        # ----------------------------------------------------------------------
        # üü¶ 10. Ëã•È¢ÑÊµãÊú¨‰ΩìÊÑüÁü•ÔºåÂàôÊ†πÊçÆ‰ªªÂä°Á±ªÂûãÂèñ GT proprioception
        # ----------------------------------------------------------------------
        if self.predict_proprioception:
            if self.task_name == "umi":
                # UMI ‰ΩøÁî® axis-angle rot
                gt_properception = proprioception_input[
                    "robot0_eef_rot_axis_angle_wrt_start_pred"
                ]

            elif self.task_name == "toolhang":
                # toolhang ÊãºÊé• eef pos + quat + gripper Áä∂ÊÄÅ
                gt_properception = torch.cat([
                    proprioception_input['robot0_eef_pos_pred'],
                    proprioception_input['robot0_eef_quat_pred'],
                    proprioception_input['robot0_gripper_qpos_pred'],
                ], dim=-1)

            else:
                raise NotImplementedError

            # ------------------------------------------------------------------
            # üü¶ 11. ËÆ°ÁÆó LossÔºöËßÜÈ¢ë + Âä®‰Ωú + wrist + proprioception
            # ------------------------------------------------------------------
            if self.predict_wrist_img:
                loss, video_loss, act_loss = self.forward_loss(
                    z=z,
                    target=gt_latents,
                    mask=mask,
                    nactions=nactions,
                    task_mode=task_mode,
                    gt_wrist_latents=gt_wrist_latents,
                    gt_properception=gt_properception,
                    text_latents=text_latents,
                )
            else:
                loss, video_loss, act_loss = self.forward_loss(
                    z=z,
                    target=gt_latents,
                    mask=mask,
                    nactions=nactions,
                    task_mode=task_mode,
                    gt_properception=gt_properception,
                    text_latents=text_latents,
                )

        # ----------------------------------------------------------------------
        # üü¶ 12. ‰∏çÈ¢ÑÊµã proprioceptionÔºà‰ªÖËßÜÈ¢ë + wrist + Âä®‰ΩúÔºâ
        # ----------------------------------------------------------------------
        else:
            if self.predict_wrist_img:
                loss, video_loss, act_loss = self.forward_loss(
                    z=z,
                    target=gt_latents,
                    mask=mask,
                    nactions=nactions,
                    task_mode=task_mode,
                    gt_wrist_latents=gt_wrist_latents,
                    text_latents=text_latents,
                )
            else:
                loss, video_loss, act_loss = self.forward_loss(
                    z=z,
                    target=gt_latents,
                    mask=mask,
                    nactions=nactions,
                    task_mode=task_mode,
                    text_latents=text_latents,
                )

        # ËøîÂõûÂêÑÈ°π loss
        return loss, video_loss, act_loss


    def sample_tokens(
        self,
        bsz,
        cond,
        text_latents=None,
        num_iter=64,
        cfg=1.0,
        cfg_schedule="linear",
        temperature=1.0,
        progress=False,
        history_nactions=None,
        nactions=None,
        proprioception_input={},
        task_mode=None,
        vae_model=None,
        x=None,
    ):
        self.device = cond.device
        B, T, C, H, W = cond.size()
        cond = rearrange(cond, "b t c h w -> (b t) c h w")
        cond = self.patchify(cond)
        cond = rearrange(
            cond, "(b t) seq_len c -> b t seq_len c", b=B
        )

        # ========= Proprioception =========
        if self.use_proprioception:
            if "second_image_z" in proprioception_input:
                proprioception_input["second_image_z"] = rearrange(
                    proprioception_input["second_image_z"], "b t c h w -> (b t) c h w"
                )
                proprioception_input["second_image_z"] = self.patchify(
                    proprioception_input["second_image_z"]
                )
                proprioception_input["second_image_z"] = rearrange(
                    proprioception_input["second_image_z"],
                    "(b t) seq_len c -> b t seq_len c",
                    b=B,
                )

        if text_latents is not None and hasattr(self, "text_proj_cond"):
            if self.language_emb_model_type == 1:
                text_latents = self.text_proj_cond(text_latents)

        # ========= Mask =========
        if task_mode == "inverse_model":
            x = rearrange(x, "b t c h w -> (b t) c h w")
            x = self.patchify(x)
            tokens = rearrange(
                x, "(b t) seq_len c -> b t seq_len c", b=B
            )
            mask = torch.zeros(bsz, self.n_frames, self.seq_len).to(self.device)
        else:
            # init and sample generation orders
            tokens = torch.zeros(
                bsz, self.n_frames, self.seq_len, self.token_embed_dim
            ).to(self.device)
            mask = torch.ones(bsz, self.n_frames, self.seq_len).to(self.device)
            if self.predict_wrist_img:
                proprioception_input["pred_second_image_z"] = torch.zeros(
                    bsz, self.n_frames, self.seq_len, self.token_embed_dim
                ).to(self.device)

        # ========= Sample Orders =========
        orders = self.sample_orders(bsz)

        indices = list(range(num_iter))
        if progress:
            indices = tqdm(indices)

        # ========= Predict Video =========
        if self.predict_video:
            for step in indices:
                cur_tokens = tokens.clone()

                if self.predict_wrist_img:
                    cur_wrist_tokens = proprioception_input[
                        "pred_second_image_z"
                    ].clone()

                x = self.forward_mae_encoder(
                    tokens,
                    mask,
                    cond,
                    text_latents,
                    history_nactions=history_nactions,
                    nactions=nactions,
                    task_mode=task_mode,
                    proprioception_input=proprioception_input,
                )
                z = self.forward_mae_decoder(x, mask)

                if self.predict_action:
                    act_cfg = 1.0
                    sampled_token_latent_act = self.diffactloss.sample(
                        z, temperature, cfg=act_cfg, text_latents=text_latents
                    )
                else:
                    sampled_token_latent_act = None

                # ========= Predict action and return if task_mode is inverse_model or policy_model=========
                if task_mode == "inverse_model" or task_mode == "policy_model":
                    if sampled_token_latent_act is None:
                        # If predict_action is False, create a dummy action tensor
                        # with the expected shape for the action dimension
                        action_dim = self.diffactloss.target_channels if hasattr(self, 'diffactloss') else 7  # fallback to typical action dim
                        sampled_token_latent_act = torch.zeros(
                            bsz, self.n_frames, action_dim
                        ).to(self.device)
                    return None, sampled_token_latent_act

                # ========= Mask Ratio =========
                # mask ratio for the next round, following MaskGIT and MAGE.
                mask_ratio = np.cos(math.pi / 2.0 * (step + 1) / num_iter)
                mask_len = torch.Tensor([np.floor(self.seq_len * mask_ratio)]).to(
                    self.device
                )

                # take the first frame mask
                mask_ = mask[:, 0]

                # masks out at least one for the next iteration
                mask_len = torch.maximum(
                    torch.Tensor([1]).to(self.device),
                    torch.minimum(
                        torch.sum(mask_, dim=-1, keepdims=True) - 1, mask_len
                    ),
                )

                # get masking for next iteration and locations to be predicted in this iteration
                mask_next = mask_by_order(
                    mask_len[0], orders, bsz, self.seq_len, self.device
                )

                ## expand mask_next to all frames
                mask_next = mask_next.unsqueeze(1).expand(-1, T, -1)
                mask_next = rearrange(mask_next, "b t s -> b (t s)")
                mask = rearrange(mask, "b t s -> b (t s)")

                if step >= num_iter - 1:
                    mask_to_pred = mask[:bsz].bool()
                else:
                    mask_to_pred = torch.logical_xor(
                        mask[:bsz].bool(), mask_next.bool()
                    )
                mask = mask_next
                mask = rearrange(mask, "b (t s) -> b t s", t=self.n_frames)

                if not cfg == 1.0:
                    mask_to_pred = torch.cat([mask_to_pred, mask_to_pred], dim=0)

                # sample token latents for this step
                z = z[mask_to_pred.nonzero(as_tuple=True)]
                # cfg schedule follow Muse
                if cfg_schedule == "linear":
                    cfg_iter = (
                        1 + (cfg - 1) * (self.seq_len - mask_len[0]) / self.seq_len
                    )
                elif cfg_schedule == "constant":
                    cfg_iter = cfg
                else:
                    raise NotImplementedError

                sampled_token_latent = self.diffloss.sample(
                    z, temperature, cfg_iter, text_latents=text_latents
                )

                if not cfg == 1.0:
                    sampled_token_latent, _ = sampled_token_latent.chunk(
                        2, dim=0
                    )  # Remove null class samples
                    mask_to_pred, _ = mask_to_pred.chunk(2, dim=0)

                cur_tokens = rearrange(cur_tokens, "b t s c -> b (t s) c")
                cur_tokens[mask_to_pred.nonzero(as_tuple=True)] = sampled_token_latent
                cur_tokens = rearrange(
                    cur_tokens, "b (t s) c -> b t s c", t=self.n_frames
                )
                tokens = cur_tokens.clone()

                # ========= Predict Wrist Image =========
                if self.predict_wrist_img:
                    sampled_wrist_token_latent = self.diffloss_wrist.sample(
                        z, temperature, cfg_iter, text_latents=text_latents
                    )

                    if not cfg == 1.0:
                        sampled_wrist_token_latent, _ = (
                            sampled_wrist_token_latent.chunk(2, dim=0)
                        )  # Remove null class samples

                    cur_wrist_tokens = rearrange(
                        cur_wrist_tokens, "b t s c -> b (t s) c"
                    )
                    cur_wrist_tokens[mask_to_pred.nonzero(as_tuple=True)] = (
                        sampled_wrist_token_latent
                    )
                    cur_wrist_tokens = rearrange(
                        cur_wrist_tokens, "b (t s) c -> b t s c", t=self.n_frames
                    )
                    proprioception_input["pred_second_image_z"] = (
                        cur_wrist_tokens.clone()
                    )

            # ========= Unpatchify =========
            tokens = rearrange(tokens, "b t s c -> (b t) s c")
            tokens = self.unpatchify(tokens)
            # tokens = rearrange(tokens, '(b t) c h w -> b t c h w', b=B)

            if self.predict_wrist_img:
                wrist_tokens = rearrange(
                    proprioception_input["pred_second_image_z"], "b t s c -> (b t) s c"
                )
                wrist_tokens = self.unpatchify(wrist_tokens)

        else:
            raise NotImplementedError

        if self.predict_wrist_img:
            return wrist_tokens, sampled_token_latent_act
        else:
            return tokens, sampled_token_latent_act


def mar_tiny(**kwargs):
    model = MAR(
        encoder_embed_dim=768,
        encoder_depth=3,
        encoder_num_heads=6,
        decoder_embed_dim=768,
        decoder_depth=3,
        decoder_num_heads=6,
        mlp_ratio=4,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs
    )
    return model


def mar_small(**kwargs):
    model = MAR(
        encoder_embed_dim=768,
        encoder_depth=6,
        encoder_num_heads=6,
        decoder_embed_dim=768,
        decoder_depth=6,
        decoder_num_heads=6,
        mlp_ratio=4,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs
    )
    return model


def mar_base(**kwargs):
    model = MAR(
        encoder_embed_dim=768,
        encoder_depth=12,
        encoder_num_heads=12,
        decoder_embed_dim=768,
        decoder_depth=12,
        decoder_num_heads=12,
        mlp_ratio=4,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs
    )
    return model


def mar_large(**kwargs):
    model = MAR(
        encoder_embed_dim=1024,
        encoder_depth=16,
        encoder_num_heads=16,
        decoder_embed_dim=1024,
        decoder_depth=16,
        decoder_num_heads=16,
        mlp_ratio=4,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs
    )
    return model


def mar_huge(**kwargs):
    model = MAR(
        encoder_embed_dim=1280,
        encoder_depth=20,
        encoder_num_heads=16,
        decoder_embed_dim=1280,
        decoder_depth=20,
        decoder_num_heads=16,
        mlp_ratio=4,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs
    )
    return model
